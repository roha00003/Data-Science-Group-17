{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "cfd3678cd0b97d14",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-02T14:07:28.142375Z",
     "start_time": "2025-07-02T14:07:28.137531Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Overall Settings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3864188eef6516ce"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 8 threads for model training.\n",
      "Models will be saved in: ../saved_models_filtered_xg_final_split_5\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"../data/data.csv\"\n",
    "MODEL_DIR = \"../saved_models_filtered_xg_final_split_5\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=False)\n",
    "os.makedirs(os.path.join(MODEL_DIR, \"feature_importance\"))\n",
    "\n",
    "NUMBER_OF_ROWS = None # None means all rows\n",
    "\n",
    "N_ESTIMATORS = 150\n",
    "MAX_DEPTH = 5\n",
    "FILTER_OUTLIERS_FAC = 0.70\n",
    "SEED = 42\n",
    "NUMBER_OF_THREADS = os.cpu_count()\n",
    "\n",
    "print(f\"Using {NUMBER_OF_THREADS} threads for model training.\")\n",
    "print(f\"Models will be saved in: {MODEL_DIR}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-03T09:33:53.114049Z",
     "start_time": "2025-07-03T09:33:53.108453Z"
    }
   },
   "id": "74299905f66d85f2",
   "execution_count": 52
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define Inputs\n",
    "We decided to have one mandatory Input with the \"CCSR Procedure Code\" and many other optional inputs.\n",
    "For each combination of the optional inputs and the one mandatory input we have to train a random forrest models."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aaa9a0274af8f9e6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "optional_features = ['Age Group', 'Gender', 'Race', 'Ethnicity']\n",
    "base_feature = ['CCSR Procedure Code', 'Type of Admission']\n",
    "all_combinations = []\n",
    "\n",
    "for r in range(len(optional_features) + 1):\n",
    "    for combo in itertools.combinations(optional_features, r):\n",
    "        all_combinations.append(base_feature + list(combo))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-03T09:33:53.797778Z",
     "start_time": "2025-07-03T09:33:53.793916Z"
    }
   },
   "id": "a3cdad947b61e1c",
   "execution_count": 53
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define Outputs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aeff9c9fa3e34190"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "targets = ['Total Costs', 'Length of Stay', 'APR Risk of Mortality']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-03T09:33:54.497764Z",
     "start_time": "2025-07-03T09:33:54.495939Z"
    }
   },
   "id": "c9a4fcda6710ca5",
   "execution_count": 54
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading the Data\n",
    "Now we load our preprocessed data and clean some parts up.\n",
    "We also encode the \"APR Risk of Mortality\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72761db097d841c5"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of loaded rows: 1239850\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_PATH, dtype=str, low_memory=False, nrows=NUMBER_OF_ROWS)\n",
    "\n",
    "# make numbers correct\n",
    "df['Total Costs'] = df['Total Costs'].astype(float)\n",
    "#df['Total Charges'] = df['Total Charges'].astype(float)\n",
    "# Replace \"120 +\" with 140 and convert to float\n",
    "df['Length of Stay'] = df['Length of Stay'].replace(\"120 +\", \"140\").astype(float)\n",
    "\n",
    "# Encode the risk of mortality\n",
    "mortality_encoder = LabelEncoder()\n",
    "df['APR Risk of Mortality'] = mortality_encoder.fit_transform(df['APR Risk of Mortality'])\n",
    "\n",
    "# Print the number of loaded rows\n",
    "print(f\"Number of loaded rows: {len(df)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-03T09:33:58.051101Z",
     "start_time": "2025-07-03T09:33:55.202045Z"
    }
   },
   "id": "5466fdf02781ff3e",
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after filtering: 724406\n"
     ]
    }
   ],
   "source": [
    "# Python\n",
    "def filter_outliers(df, feature):\n",
    "    median = df[feature].median()\n",
    "    lower_bound = median * (1 - FILTER_OUTLIERS_FAC)\n",
    "    upper_bound = median * (1 + FILTER_OUTLIERS_FAC)\n",
    "    return df[(df[feature] >= lower_bound) & (df[feature] <= upper_bound)]\n",
    "\n",
    "filtered_dfs = []\n",
    "\n",
    "# Iterate over unique combinations of procedure code and type of admission\n",
    "for procedure_code, admission_type in df[['CCSR Procedure Code', 'Type of Admission']].drop_duplicates().itertuples(index=False):\n",
    "    subset = df[(df['CCSR Procedure Code'] == procedure_code) & (df['Type of Admission'] == admission_type)]\n",
    "    #for feature in targets:\n",
    "    #    subset = filter_outliers(subset, feature)\n",
    "    subset = filter_outliers(subset, 'Total Costs')\n",
    "    subset = filter_outliers(subset, 'Length of Stay')\n",
    "    filtered_dfs.append(subset)\n",
    "\n",
    "# Combine all filtered subsets\n",
    "filtered_df = pd.concat(filtered_dfs, ignore_index=True)\n",
    "df = filtered_df\n",
    "\n",
    "# Print the number of rows after filtering\n",
    "print(f\"Number of rows after filtering: {len(filtered_df)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-03T09:35:40.849631Z",
     "start_time": "2025-07-03T09:33:58.055268Z"
    }
   },
   "id": "e77b0184eac650e5",
   "execution_count": 56
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define the model training function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2fb98ded8b226032"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_model_xg(features):\n",
    "    try:\n",
    "        # OneHot-Encoding der Features\n",
    "        encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "        X_encoded = encoder.fit_transform(df[features])\n",
    "        X_df = pd.DataFrame(X_encoded, columns=encoder.get_feature_names_out(features))\n",
    "\n",
    "        y_df = df[targets].reset_index(drop=True)\n",
    "\n",
    "        # Move data to GPU\n",
    "        # dtrain = xgb.DMatrix(X_df, label=y_df, device='cuda')\n",
    "\n",
    "        # XGBoost-Konfiguration für maximale Performance\n",
    "        base_model = xgb.XGBRegressor(\n",
    "            n_estimators=N_ESTIMATORS,             # z. B. 100 oder 500\n",
    "            tree_method='hist',                # GPU!\n",
    "            booster='gbtree',                      # Tree Booster\n",
    "            device = 'cpu',                        # GPU!\n",
    "            max_depth=MAX_DEPTH,                          # Tiefer = komplexer\n",
    "            subsample=0.8,                         # Bagging\n",
    "            colsample_bytree=0.8,                  # Feature Sampling\n",
    "            learning_rate=0.1,                     # kleiner bei mehr Estimators\n",
    "            n_jobs = -1  # nutzt alle CPU-Kerne\n",
    "        )\n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=0.2, random_state=SEED)\n",
    "        \n",
    "        # Train the model on the training set\n",
    "        model = MultiOutputRegressor(base_model)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on the testing set\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate scores on the testing set\n",
    "        scores = {}\n",
    "        for i, target in enumerate(targets):\n",
    "            scores[f\"{target}_r2\"] = r2_score(y_test.iloc[:, i], y_pred[:, i])\n",
    "            scores[f\"{target}_mse\"] = mean_squared_error(y_test.iloc[:, i], y_pred[:, i])\n",
    "            \n",
    "        # calculate scores for training set\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        for i, target in enumerate(targets):\n",
    "            scores[f\"{target}_train_r2\"] = r2_score(y_train.iloc[:, i], y_train_pred[:, i])\n",
    "            scores[f\"{target}_train_mse\"] = mean_squared_error(y_train.iloc[:, i], y_train_pred[:, i])\n",
    "\n",
    "        # save\n",
    "        model_name = f\"{'__'.join(f.replace(' ', '_') for f in features)}.pkl\"\n",
    "        model_path = os.path.join(MODEL_DIR, model_name)\n",
    "\n",
    "        joblib.dump({\n",
    "            \"model\": model,\n",
    "            \"features\": features,\n",
    "            \"encoder\": encoder,\n",
    "            \"target_columns\": targets,\n",
    "            \"mortality_encoder\": mortality_encoder\n",
    "        }, model_path)\n",
    "\n",
    "        return {\n",
    "            \"features\": features,\n",
    "            \"model_path\": model_path,\n",
    "            **scores\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error training model for features {features}: {e}\", flush=True)\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-03T09:35:40.855713Z",
     "start_time": "2025-07-03T09:35:40.850544Z"
    }
   },
   "id": "a636b00205e22ec8",
   "execution_count": 57
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Real Random Forest Model Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68d6229694a5a29e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_model_rf(features):\n",
    "    try:\n",
    "        # OneHot-Encoding der Features\n",
    "        encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "        X_encoded = encoder.fit_transform(df[features])\n",
    "        X_df = pd.DataFrame(X_encoded, columns=encoder.get_feature_names_out(features))\n",
    "\n",
    "        y_df = df[targets].reset_index(drop=True)\n",
    "\n",
    "        # Random Forest-Konfiguration für maximale Performance\n",
    "        base_model = RandomForestRegressor(\n",
    "            n_estimators=N_ESTIMATORS,  # Number of trees\n",
    "            max_depth=MAX_DEPTH,               # Maximum depth of the trees\n",
    "            n_jobs=NUMBER_OF_THREADS,                 # Use all CPU cores\n",
    "            random_state=SEED          # Ensure reproducibility\n",
    "        )\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=0.2, random_state=SEED)\n",
    "        # Train the model on the training set\n",
    "        model = MultiOutputRegressor(base_model)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on the testing set\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate scores on the testing set\n",
    "        scores = {}\n",
    "        for i, target in enumerate(targets):\n",
    "            scores[f\"{target}_r2\"] = r2_score(y_test.iloc[:, i], y_pred[:, i])\n",
    "            scores[f\"{target}_mse\"] = mean_squared_error(y_test.iloc[:, i], y_pred[:, i])\n",
    "            \n",
    "        y_train_pred = model.predict(X_train)\n",
    "        for i, target in enumerate(targets):\n",
    "            scores[f\"{target}_train_r2\"] = r2_score(y_train.iloc[:, i], y_train_pred[:, i])\n",
    "            scores[f\"{target}_train_mse\"] = mean_squared_error(y_train.iloc[:, i], y_train_pred[:, i])\n",
    "\n",
    "        # Modell speichern\n",
    "        model_name = f\"{'__'.join(f.replace(' ', '_') for f in features)}.pkl\"\n",
    "        model_path = os.path.join(MODEL_DIR, model_name)\n",
    "        \n",
    "        # feature importance speichern\n",
    "        feature_importances = model.estimators_[0].feature_importances_\n",
    "        feature_importances_df = pd.DataFrame({\n",
    "            'feature': X_df.columns,\n",
    "            'importance': feature_importances\n",
    "        }).sort_values(by='importance', ascending=False)\n",
    "        feature_importances_path = os.path.join(MODEL_DIR, \"feature_importance\", f\"{model_name.replace('.pkl', '_feature_importances.csv')}\")\n",
    "        feature_importances_df.to_csv(feature_importances_path, index=False)\n",
    "\n",
    "        joblib.dump({\n",
    "            \"model\": model,\n",
    "            \"features\": features,\n",
    "            \"encoder\": encoder,\n",
    "            \"target_columns\": targets,\n",
    "            \"mortality_encoder\": mortality_encoder\n",
    "        }, model_path)\n",
    "\n",
    "        return {\n",
    "            \"features\": features,\n",
    "            \"model_path\": model_path,\n",
    "            **scores\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error training model for features {features}: {e}\", flush=True)\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-03T09:35:40.862627Z",
     "start_time": "2025-07-03T09:35:40.857172Z"
    }
   },
   "id": "34a5b2a9dd95d5e0",
   "execution_count": 58
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Use Multithreading for the model training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "859a86697f16837f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|██████████| 16/16 [05:59<00:00, 22.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Models saved in: ../saved_models_filtered_xg_final_split_5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_model = train_model_xg  # Change to train_model_xg for XGBoost\n",
    "\n",
    "#results = Parallel(n_jobs=NUMBER_OF_THREADS)(\n",
    "#    delayed(train_model)(feature_comb) for feature_comb in tqdm(all_combinations, desc=\"Training Models\")\n",
    "#)\n",
    "\n",
    "results = []\n",
    "for feature_comb in tqdm(all_combinations, desc=\"Training Models\"):\n",
    "    result = train_model(feature_comb)\n",
    "    if result is not None:\n",
    "        results.append(result)\n",
    "\n",
    "# Save summary\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(os.path.join(MODEL_DIR, \"model_overview.csv\"), index=False)\n",
    "print(\"\\n Models saved in:\", MODEL_DIR)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-03T09:41:40.353059Z",
     "start_time": "2025-07-03T09:35:40.863174Z"
    }
   },
   "id": "e246ed74e441e7d6",
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1b535f6b9a9ecf01"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
