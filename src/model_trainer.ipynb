{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "cfd3678cd0b97d14",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-28T12:09:08.577526Z",
     "start_time": "2025-06-28T12:09:08.574747Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Overall Settings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3864188eef6516ce"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: '../saved_models_filtered_rf_final'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileExistsError\u001B[39m                           Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[21]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m DATA_PATH = \u001B[33m\"\u001B[39m\u001B[33m../data/data.csv\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m      2\u001B[39m MODEL_DIR = \u001B[33m\"\u001B[39m\u001B[33m../saved_models_filtered_rf_final\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m \u001B[43mos\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmakedirs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mMODEL_DIR\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexist_ok\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m      4\u001B[39m os.makedirs(os.path.join(MODEL_DIR, \u001B[33m\"\u001B[39m\u001B[33mfeature_importance\u001B[39m\u001B[33m\"\u001B[39m))\n\u001B[32m      6\u001B[39m NUMBER_OF_ROWS = \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;66;03m# None means all rows\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen os>:225\u001B[39m, in \u001B[36mmakedirs\u001B[39m\u001B[34m(name, mode, exist_ok)\u001B[39m\n",
      "\u001B[31mFileExistsError\u001B[39m: [Errno 17] File exists: '../saved_models_filtered_rf_final'"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"../data/data.csv\"\n",
    "MODEL_DIR = \"../saved_models_filtered_rf_final\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=False)\n",
    "os.makedirs(os.path.join(MODEL_DIR, \"feature_importance\"))\n",
    "\n",
    "NUMBER_OF_ROWS = None # None means all rows\n",
    "\n",
    "N_ESTIMATORS = 100\n",
    "MAX_DEPTH = 5\n",
    "FILTER_OUTLIERS_FAC = 0.65\n",
    "SEED = 42\n",
    "NUMBER_OF_THREADS = os.cpu_count()\n",
    "\n",
    "print(f\"Using {NUMBER_OF_THREADS} threads for model training.\")\n",
    "print(f\"Models will be saved in: {MODEL_DIR}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-28T12:09:09.707760Z",
     "start_time": "2025-06-28T12:09:09.549928Z"
    }
   },
   "id": "74299905f66d85f2",
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define Inputs\n",
    "We decided to have one mandatory Input with the \"CCSR Procedure Code\" and many other optional inputs.\n",
    "For each combination of the optional inputs and the one mandatory input we have to train a random forrest models."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aaa9a0274af8f9e6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "optional_features = ['Age Group', 'Gender', 'Race', 'Ethnicity']\n",
    "base_feature = ['CCSR Procedure Code', 'Type of Admission']\n",
    "all_combinations = []\n",
    "\n",
    "for r in range(len(optional_features) + 1):\n",
    "    for combo in itertools.combinations(optional_features, r):\n",
    "        all_combinations.append(base_feature + list(combo))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-28T12:09:12.187916Z",
     "start_time": "2025-06-28T12:09:12.184120Z"
    }
   },
   "id": "a3cdad947b61e1c",
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define Outputs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aeff9c9fa3e34190"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "targets = ['Total Costs', 'Length of Stay', 'APR Risk of Mortality']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-28T12:09:13.155342Z",
     "start_time": "2025-06-28T12:09:13.153490Z"
    }
   },
   "id": "c9a4fcda6710ca5",
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading the Data\n",
    "Now we load our preprocessed data and clean some parts up.\n",
    "We also encode the \"APR Risk of Mortality\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72761db097d841c5"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of loaded rows: 10\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_PATH, dtype=str, low_memory=False, nrows=NUMBER_OF_ROWS)\n",
    "\n",
    "# make numbers correct\n",
    "df['Total Costs'] = df['Total Costs'].astype(float)\n",
    "#df['Total Charges'] = df['Total Charges'].astype(float)\n",
    "# Replace \"120 +\" with 140 and convert to float\n",
    "df['Length of Stay'] = df['Length of Stay'].replace(\"120 +\", \"140\").astype(float)\n",
    "\n",
    "# Encode the risk of mortality\n",
    "mortality_encoder = LabelEncoder()\n",
    "df['APR Risk of Mortality'] = mortality_encoder.fit_transform(df['APR Risk of Mortality'])\n",
    "\n",
    "# Print the number of loaded rows\n",
    "print(f\"Number of loaded rows: {len(df)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-28T12:09:14.305982Z",
     "start_time": "2025-06-28T12:09:14.299344Z"
    }
   },
   "id": "5466fdf02781ff3e",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after filtering: 7\n"
     ]
    }
   ],
   "source": [
    "# Python\n",
    "def filter_outliers(df, feature):\n",
    "    median = df[feature].median()\n",
    "    lower_bound = median * (1 - FILTER_OUTLIERS_FAC)\n",
    "    upper_bound = median * (1 + FILTER_OUTLIERS_FAC)\n",
    "    return df[(df[feature] >= lower_bound) & (df[feature] <= upper_bound)]\n",
    "\n",
    "filtered_dfs = []\n",
    "\n",
    "# Iterate over unique combinations of procedure code and type of admission\n",
    "for procedure_code, admission_type in df[['CCSR Procedure Code', 'Type of Admission']].drop_duplicates().itertuples(index=False):\n",
    "    subset = df[(df['CCSR Procedure Code'] == procedure_code) & (df['Type of Admission'] == admission_type)]\n",
    "    #for feature in targets:\n",
    "    #    subset = filter_outliers(subset, feature)\n",
    "    subset = filter_outliers(subset, 'Total Costs')\n",
    "    filtered_dfs.append(subset)\n",
    "\n",
    "# Combine all filtered subsets\n",
    "filtered_df = pd.concat(filtered_dfs, ignore_index=True)\n",
    "df = filtered_df\n",
    "\n",
    "# Print the number of rows after filtering\n",
    "print(f\"Number of rows after filtering: {len(filtered_df)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-28T12:09:14.818639Z",
     "start_time": "2025-06-28T12:09:14.810748Z"
    }
   },
   "id": "e77b0184eac650e5",
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define the model training function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2fb98ded8b226032"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_model_xg(features):\n",
    "    try:\n",
    "        # OneHot-Encoding der Features\n",
    "        encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "        X_encoded = encoder.fit_transform(df[features])\n",
    "        X_df = pd.DataFrame(X_encoded, columns=encoder.get_feature_names_out(features))\n",
    "\n",
    "        y_df = df[targets].reset_index(drop=True)\n",
    "\n",
    "        # Move data to GPU\n",
    "        # dtrain = xgb.DMatrix(X_df, label=y_df, device='cuda')\n",
    "\n",
    "        # XGBoost-Konfiguration für maximale Performance\n",
    "        base_model = xgb.XGBRegressor(\n",
    "            n_estimators=N_ESTIMATORS,             # z. B. 100 oder 500\n",
    "            tree_method='hist',                # GPU!\n",
    "            booster='gbtree',                      # Tree Booster\n",
    "            device = 'cpu',                        # GPU!\n",
    "            max_depth=5,                          # Tiefer = komplexer\n",
    "            subsample=0.8,                         # Bagging\n",
    "            colsample_bytree=0.8,                  # Feature Sampling\n",
    "            learning_rate=0.1,                     # kleiner bei mehr Estimators\n",
    "            n_jobs = -1  # nutzt alle CPU-Kerne\n",
    "        )\n",
    "\n",
    "        # MultiOutputRegressor für 4 Zielspalten\n",
    "        model = MultiOutputRegressor(base_model)\n",
    "        model.fit(X_df.to_numpy(), y_df.to_numpy())\n",
    "\n",
    "        # Vorhersage und Bewertung\n",
    "        y_pred = model.predict(X_df)\n",
    "\n",
    "        scores = {}\n",
    "        for i, target in enumerate(targets):\n",
    "            scores[f\"{target}_r2\"] = r2_score(y_df.iloc[:, i], y_pred[:, i])\n",
    "            scores[f\"{target}_mse\"] = mean_squared_error(y_df.iloc[:, i], y_pred[:, i])\n",
    "\n",
    "        # Modell speichern\n",
    "        model_name = f\"{'__'.join(f.replace(' ', '_') for f in features)}.pkl\"\n",
    "        model_path = os.path.join(MODEL_DIR, model_name)\n",
    "\n",
    "        joblib.dump({\n",
    "            \"model\": model,\n",
    "            \"features\": features,\n",
    "            \"encoder\": encoder,\n",
    "            \"target_columns\": targets,\n",
    "            \"mortality_encoder\": mortality_encoder\n",
    "        }, model_path)\n",
    "\n",
    "        return {\n",
    "            \"features\": features,\n",
    "            \"model_path\": model_path,\n",
    "            **scores\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error training model for features {features}: {e}\", flush=True)\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-28T12:09:15.990877Z",
     "start_time": "2025-06-28T12:09:15.985538Z"
    }
   },
   "id": "a636b00205e22ec8",
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Real Random Forest Model Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68d6229694a5a29e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_model_rf(features):\n",
    "    try:\n",
    "        # OneHot-Encoding der Features\n",
    "        encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "        X_encoded = encoder.fit_transform(df[features])\n",
    "        X_df = pd.DataFrame(X_encoded, columns=encoder.get_feature_names_out(features))\n",
    "\n",
    "        y_df = df[targets].reset_index(drop=True)\n",
    "\n",
    "        # Random Forest-Konfiguration für maximale Performance\n",
    "        base_model = RandomForestRegressor(\n",
    "            n_estimators=N_ESTIMATORS,  # Number of trees\n",
    "            max_depth=MAX_DEPTH,               # Maximum depth of the trees\n",
    "            n_jobs=NUMBER_OF_THREADS,                 # Use all CPU cores\n",
    "            random_state=SEED          # Ensure reproducibility\n",
    "        )\n",
    "\n",
    "        # MultiOutputRegressor für 4 Zielspalten\n",
    "        model = MultiOutputRegressor(base_model)\n",
    "        model.fit(X_df.to_numpy(), y_df.to_numpy())\n",
    "\n",
    "        # Vorhersage und Bewertung\n",
    "        y_pred = model.predict(X_df.to_numpy())\n",
    "\n",
    "        scores = {}\n",
    "        for i, target in enumerate(targets):\n",
    "            scores[f\"{target}_r2\"] = r2_score(y_df.iloc[:, i], y_pred[:, i])\n",
    "            scores[f\"{target}_mse\"] = mean_squared_error(y_df.iloc[:, i], y_pred[:, i])\n",
    "\n",
    "        # Modell speichern\n",
    "        model_name = f\"{'__'.join(f.replace(' ', '_') for f in features)}.pkl\"\n",
    "        model_path = os.path.join(MODEL_DIR, model_name)\n",
    "        \n",
    "        # feature importance speichern\n",
    "        feature_importances = model.estimators_[0].feature_importances_\n",
    "        feature_importances_df = pd.DataFrame({\n",
    "            'feature': X_df.columns,\n",
    "            'importance': feature_importances\n",
    "        }).sort_values(by='importance', ascending=False)\n",
    "        feature_importances_path = os.path.join(MODEL_DIR, f\"{model_name.replace('.pkl', '_feature_importances.csv')}\")\n",
    "        feature_importances_df.to_csv(feature_importances_path, index=False)\n",
    "\n",
    "        joblib.dump({\n",
    "            \"model\": model,\n",
    "            \"features\": features,\n",
    "            \"encoder\": encoder,\n",
    "            \"target_columns\": targets,\n",
    "            \"mortality_encoder\": mortality_encoder\n",
    "        }, model_path)\n",
    "\n",
    "        return {\n",
    "            \"features\": features,\n",
    "            \"model_path\": model_path,\n",
    "            **scores\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error training model for features {features}: {e}\", flush=True)\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-28T12:09:17.390088Z",
     "start_time": "2025-06-28T12:09:17.383296Z"
    }
   },
   "id": "34a5b2a9dd95d5e0",
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Use Multithreading for the model training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "859a86697f16837f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|██████████| 16/16 [00:03<00:00,  4.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Models saved in: ../saved_models_filtered_rf_final\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_model = train_model_rf  # Change to train_model_xg for XGBoost\n",
    "\n",
    "#results = Parallel(n_jobs=NUMBER_OF_THREADS)(\n",
    "#    delayed(train_model)(feature_comb) for feature_comb in tqdm(all_combinations, desc=\"Training Models\")\n",
    "#)\n",
    "\n",
    "results = []\n",
    "for feature_comb in tqdm(all_combinations, desc=\"Training Models\"):\n",
    "    result = train_model(feature_comb)\n",
    "    if result is not None:\n",
    "        results.append(result)\n",
    "\n",
    "# Save summary\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(os.path.join(MODEL_DIR, \"model_overview.csv\"), index=False)\n",
    "print(\"\\n Models saved in:\", MODEL_DIR)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-28T12:09:22.654441Z",
     "start_time": "2025-06-28T12:09:18.663685Z"
    }
   },
   "id": "e246ed74e441e7d6",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1b535f6b9a9ecf01"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
