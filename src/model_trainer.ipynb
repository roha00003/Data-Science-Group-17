{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "cfd3678cd0b97d14",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-22T18:00:26.298861Z",
     "start_time": "2025-06-22T18:00:26.290746Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Overall Settings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3864188eef6516ce"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/data.csv\"\n",
    "MODEL_DIR = \"../saved_models\"\n",
    "NUMBER_OF_ROWS = None\n",
    "N_ESTIMATORS = 100\n",
    "SEED = 42\n",
    "NUMBER_OF_THREADS = os.cpu_count()\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-22T18:00:27.415754Z",
     "start_time": "2025-06-22T18:00:27.410164Z"
    }
   },
   "id": "74299905f66d85f2",
   "execution_count": 71
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define Inputs\n",
    "We decided to have one mandatory Input with the \"CCSR Procedure Code\" and many other optional inputs.\n",
    "For each combination of the optional inputs and the one mandatory input we have to train a random forrest models."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aaa9a0274af8f9e6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "optional_features = ['Age Group', 'Gender', 'Race', 'Ethnicity']\n",
    "base_feature = ['CCSR Procedure Code', ]\n",
    "all_combinations = []\n",
    "\n",
    "for r in range(len(optional_features) + 1):\n",
    "    for combo in itertools.combinations(optional_features, r):\n",
    "        all_combinations.append(base_feature + list(combo))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-22T18:00:28.172513Z",
     "start_time": "2025-06-22T18:00:28.169657Z"
    }
   },
   "id": "a3cdad947b61e1c",
   "execution_count": 72
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define Outputs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aeff9c9fa3e34190"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "targets = ['Total Costs', 'Total Charges', 'Length of Stay', 'APR Risk of Mortality']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-22T18:00:28.934347Z",
     "start_time": "2025-06-22T18:00:28.932457Z"
    }
   },
   "id": "c9a4fcda6710ca5",
   "execution_count": 73
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading the Data\n",
    "Now we load our preprocessed data and clean some parts up.\n",
    "We also encode the \"APR Risk of Mortality\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72761db097d841c5"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of loaded rows: 1239850\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_PATH, dtype=str, low_memory=False, nrows=NUMBER_OF_ROWS)\n",
    "\n",
    "# make numbers correct\n",
    "df['Total Costs'] = df['Total Costs'].astype(float)\n",
    "df['Total Charges'] = df['Total Charges'].astype(float)\n",
    "# Replace \"120 +\" with 140 and convert to float\n",
    "df['Length of Stay'] = df['Length of Stay'].replace(\"120 +\", \"140\").astype(float)\n",
    "\n",
    "# Encode the risk of mortality\n",
    "mortality_encoder = LabelEncoder()\n",
    "df['APR Risk of Mortality'] = mortality_encoder.fit_transform(df['APR Risk of Mortality'])\n",
    "\n",
    "# Print the number of loaded rows\n",
    "print(f\"Number of loaded rows: {len(df)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-22T18:00:32.266146Z",
     "start_time": "2025-06-22T18:00:29.772502Z"
    }
   },
   "id": "5466fdf02781ff3e",
   "execution_count": 74
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define the model training function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2fb98ded8b226032"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_model(features):\n",
    "    try:\n",
    "        # OneHot-Encoding der Features\n",
    "        encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "        X_encoded = encoder.fit_transform(df[features])\n",
    "        X_df = pd.DataFrame(X_encoded, columns=encoder.get_feature_names_out(features))\n",
    "\n",
    "        y_df = df[targets].reset_index(drop=True)\n",
    "\n",
    "        # Move data to GPU\n",
    "        # dtrain = xgb.DMatrix(X_df, label=y_df, device='cuda')\n",
    "\n",
    "        # XGBoost-Konfiguration f√ºr maximale Performance\n",
    "        base_model = xgb.XGBRegressor(\n",
    "            n_estimators=N_ESTIMATORS,             # z.‚ÄØB. 100 oder 500\n",
    "            tree_method='hist',                # GPU!\n",
    "            booster='gbtree',                      # Tree Booster\n",
    "            device = 'cpu',                        # GPU!\n",
    "            max_depth=5,                          # Tiefer = komplexer\n",
    "            subsample=0.8,                         # Bagging\n",
    "            colsample_bytree=0.8,                  # Feature Sampling\n",
    "            learning_rate=0.1,                     # kleiner bei mehr Estimators\n",
    "            n_jobs = -1  # nutzt alle CPU-Kerne\n",
    "        )\n",
    "\n",
    "        # MultiOutputRegressor f√ºr 4 Zielspalten\n",
    "        model = MultiOutputRegressor(base_model)\n",
    "        model.fit(X_df.to_numpy(), y_df.to_numpy())\n",
    "\n",
    "        # Vorhersage und Bewertung\n",
    "        y_pred = model.predict(X_df)\n",
    "\n",
    "        scores = {}\n",
    "        for i, target in enumerate(targets):\n",
    "            scores[f\"{target}_r2\"] = r2_score(y_df.iloc[:, i], y_pred[:, i])\n",
    "            scores[f\"{target}_mse\"] = mean_squared_error(y_df.iloc[:, i], y_pred[:, i])\n",
    "\n",
    "        # Modell speichern\n",
    "        model_name = f\"{'__'.join(f.replace(' ', '_') for f in features)}.pkl\"\n",
    "        model_path = os.path.join(MODEL_DIR, model_name)\n",
    "\n",
    "        joblib.dump({\n",
    "            \"model\": model,\n",
    "            \"features\": features,\n",
    "            \"encoder\": encoder,\n",
    "            \"target_columns\": targets,\n",
    "            \"mortality_encoder\": mortality_encoder\n",
    "        }, model_path)\n",
    "\n",
    "        return {\n",
    "            \"features\": features,\n",
    "            \"model_path\": model_path,\n",
    "            **scores\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error training model for features {features}: {e}\", flush=True)\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-22T18:01:29.268498Z",
     "start_time": "2025-06-22T18:01:29.261259Z"
    }
   },
   "id": "a636b00205e22ec8",
   "execution_count": 77
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Use Multithreading for the model training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "859a86697f16837f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Models:   0%|          | 0/16 [00:00<?, ?it/s]\u001B[A\n",
      "Training Models:   6%|‚ñã         | 1/16 [00:34<08:37, 34.51s/it]\u001B[A\n",
      "Training Models:  12%|‚ñà‚ñé        | 2/16 [01:13<08:40, 37.21s/it]\u001B[A\n",
      "Training Models:  19%|‚ñà‚ñâ        | 3/16 [01:52<08:15, 38.10s/it]\u001B[A\n",
      "Training Models:  25%|‚ñà‚ñà‚ñå       | 4/16 [02:32<07:46, 38.91s/it]\u001B[A\n",
      "Training Models:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [03:13<07:14, 39.52s/it]\u001B[A\n",
      "Training Models:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [03:57<06:50, 41.06s/it]\u001B[A\n",
      "Training Models:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [04:42<06:20, 42.27s/it]\u001B[A\n",
      "Training Models:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [05:26<05:42, 42.87s/it]\u001B[A\n",
      "Training Models:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [06:10<05:02, 43.21s/it]\u001B[A\n",
      "Training Models:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [06:54<04:21, 43.54s/it]\u001B[A\n",
      "Training Models:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [07:40<03:40, 44.11s/it]\u001B[A\n",
      "Training Models:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [08:26<02:59, 44.82s/it]\u001B[A\n",
      "Training Models:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [09:12<02:15, 45.15s/it]\u001B[A\n",
      "Training Models:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [09:59<01:31, 45.61s/it]\u001B[A\n",
      "Training Models:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [10:44<00:45, 45.63s/it]\u001B[A\n",
      "Training Models: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [11:33<00:00, 43.34s/it]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Models saved in: ../saved_models\n",
      "üìÑ Summary saved as: model_overview.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#results = Parallel(n_jobs=NUMBER_OF_THREADS)(\n",
    "#    delayed(train_model)(feature_comb) for feature_comb in tqdm(all_combinations, desc=\"Training Models\")\n",
    "#)\n",
    "\n",
    "results = []\n",
    "for feature_comb in tqdm(all_combinations, desc=\"Training Models\"):\n",
    "    result = train_model(feature_comb)\n",
    "    if result is not None:\n",
    "        results.append(result)\n",
    "\n",
    "# Save summary\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(os.path.join(MODEL_DIR, \"model_overview.csv\"), index=False)\n",
    "print(\"\\nüì¶ Models saved in:\", MODEL_DIR)\n",
    "print(\"üìÑ Summary saved as: model_overview.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-22T18:13:03.781Z",
     "start_time": "2025-06-22T18:01:30.340047Z"
    }
   },
   "id": "e246ed74e441e7d6",
   "execution_count": 78
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1b535f6b9a9ecf01"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
